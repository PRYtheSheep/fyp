{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f9a306-ce41-4ff3-bb63-7f828d65d67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRYth\\anaconda3\\envs\\fyp\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "C:\\Users\\PRYth\\anaconda3\\envs\\fyp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "# from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "func_to_enable_grad = '_sample'\n",
    "setattr(LlavaForConditionalGeneration, func_to_enable_grad, torch.enable_grad(getattr(LlavaForConditionalGeneration, func_to_enable_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c76156-9194-45c1-b771-5db7c48864db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:27<00:00,  9.24s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  \n",
      "What is this animal? ASSISTANT: This animal is a dog, specifically a\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    offload_state_dict=True\n",
    ")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    quantization_config=quant_config,\n",
    "    attn_implementation = \"eager\"\n",
    ").to(0)\n",
    "\n",
    "#--------------------------------------------------\n",
    "model.vision_tower.config.output_attentions = True\n",
    "\n",
    "# set hooks to get attention weights\n",
    "model.enc_attn_weights = []\n",
    "#outputs: attn_output, attn_weights, past_key_value\n",
    "def forward_hook(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.error(\n",
    "            (\"Attention weights were not returned for the encoder. \"\n",
    "            \"To enable, set output_attentions=True in the forward pass of the model. \")\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder, hooks_encoder = [], []\n",
    "for layer in model.language_model.layers:\n",
    "    hook_encoder_layer = layer.self_attn.register_forward_hook(forward_hook)\n",
    "    hooks_pre_encoder.append(hook_encoder_layer)\n",
    "\n",
    "model.enc_attn_weights_vit = []\n",
    "\n",
    "def forward_hook_image_processor(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.warning(\n",
    "            (\"Attention weights were not returned for the vision model. \"\n",
    "             \"Relevancy maps will not be calculated for the vision model. \" \n",
    "             \"To enable, set output_attentions=True in the forward pass of vision_tower. \")\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights_vit.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder_vit = []\n",
    "for layer in model.vision_tower.vision_model.encoder.layers:\n",
    "    hook_encoder_layer_vit = layer.self_attn.register_forward_hook(forward_hook_image_processor)\n",
    "    hooks_pre_encoder_vit.append(hook_encoder_layer_vit)\n",
    "#--------------------------------------------------\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "if model.language_model.config.model_type == \"gemma\":\n",
    "    eos_token_id = processor.tokenizer('<end_of_turn>', add_special_tokens=False).input_ids[0]\n",
    "else:\n",
    "    eos_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is this animal?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "image_file = r\"C:\\Users\\PRYth\\OneDrive\\Pictures\\Screenshots\\Screenshot 2025-09-24 103058.png\"\n",
    "raw_image = Image.open(image_file).convert(\"RGB\")\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=8, \n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    eos_token_id=eos_token_id\n",
    ")\n",
    "\n",
    "for h in hooks_pre_encoder:\n",
    "    h.remove()\n",
    "for h in hooks_pre_encoder_vit:\n",
    "    h.remove()\n",
    "\n",
    "print(processor.decode(output.sequences[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b194a8-8b2c-4a63-b6e5-4f3588184afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output.sequences[0] is a tensor of shape [604]\n",
    "tokens = output.sequences[0].tolist()  # convert to Python list\n",
    "torch.save(output.sequences[0], \"full_generation.pt\")\n",
    "\n",
    "# Decode each token individually\n",
    "decoded_tokens = [processor.decode([t]) for t in tokens]\n",
    "\n",
    "# Optionally, print\n",
    "for i, tok in enumerate(decoded_tokens):\n",
    "    print(f\"{i}: {repr(tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26994886-41f9-4672-9d34-cc1e96cd3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_step = conversation + [\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": processor.decode(tokens[596:601])}]}\n",
    "]\n",
    "conv_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45108c4-b765-418c-a0d0-6fc38d6717d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del processor\n",
    "del hooks_pre_encoder\n",
    "del hooks_encoder\n",
    "del prompt\n",
    "del inputs\n",
    "del output\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5187c-474e-4829-927d-96cc7c141b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf5f7b-05ab-4813-a36b-ede5a960d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation = \"eager\"\n",
    ").to(0)\n",
    "\n",
    "#--------------------------------------------------\n",
    "model.vision_tower.config.output_attentions = True\n",
    "\n",
    "# set hooks to get attention weights\n",
    "model.enc_attn_weights = []\n",
    "#outputs: attn_output, attn_weights, past_key_value\n",
    "def forward_hook(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.error(\n",
    "            (\"Attention weights were not returned for the encoder. \"\n",
    "            \"To enable, set output_attentions=True in the forward pass of the model. \")\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder, hooks_encoder = [], []\n",
    "for layer in model.language_model.layers:\n",
    "    hook_encoder_layer = layer.self_attn.register_forward_hook(forward_hook)\n",
    "    hooks_pre_encoder.append(hook_encoder_layer)\n",
    "\n",
    "model.enc_attn_weights_vit = []\n",
    "\n",
    "def forward_hook_image_processor(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.warning(\n",
    "            (\"Attention weights were not returned for the vision model. \"\n",
    "             \"Relevancy maps will not be calculated for the vision model. \" \n",
    "             \"To enable, set output_attentions=True in the forward pass of vision_tower. \")\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights_vit.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder_vit = []\n",
    "for layer in model.vision_tower.vision_model.encoder.layers:\n",
    "    hook_encoder_layer_vit = layer.self_attn.register_forward_hook(forward_hook_image_processor)\n",
    "    hooks_pre_encoder_vit.append(hook_encoder_layer_vit)\n",
    "#--------------------------------------------------\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "prompt_appended = processor.apply_chat_template(conv_step, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "inputs_appended = processor(images=raw_image, text=prompt_appended, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "print(len(model.enc_attn_weights))\n",
    "output_appended = model(**inputs_appended,\n",
    "                        use_cache=True,\n",
    "                        output_attentions=True,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                        eos_token_id=eos_token_id)\n",
    "print(len(model.enc_attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7fea3-e172-4cec-afee-fe9df7a5e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_appended.logits.shape)\n",
    "# Decode them\n",
    "topk = torch.topk(output_appended.logits[:,-2], k=1, dim=-1)\n",
    "for ids in topk.indices:\n",
    "    print(processor.tokenizer.batch_decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a83cc-286d-49c4-9293-3d06a43272f1",
   "metadata": {},
   "source": [
    "# Visualise ViT attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88826e80-2fa8-4ff7-b981-8973a3d09a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights_vit[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1101f19-ac79-424a-aa54-7296b7d843c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "attn_avg = model.enc_attn_weights_vit[3][0].mean(dim=0)\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[0, 1:]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3e24c-1587-4f49-875f-713f3039a954",
   "metadata": {},
   "source": [
    "# Visualize LLM attention \n",
    "## During first forward pass when attn weights are input_token_len by input_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d7419-b19b-4306-a602-549331c598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21212358-832a-42c3-9bf2-8b41b63a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"attn_weights has length: {len(model.enc_attn_weights)}\")\n",
    "print(f\"no. of forward passes = {len(model.enc_attn_weights)}/32 = {int(len(model.enc_attn_weights)/32)}\")\n",
    "\n",
    "# First forward pass, first layer\n",
    "print(model.enc_attn_weights[0].shape) # [batch, head, no. of tokens, no. of tokens]\n",
    "\n",
    "print(model.enc_attn_weights[51].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd8201-b121-49af-943b-a9ebddbd4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model starts generating tokens after \"...ASSISTANT:\"\n",
    "attn_avg = model.enc_attn_weights[11][0].mean(dim=0)\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[592, 5:581]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d44c2e-9a7b-44eb-887d-476b99b71417",
   "metadata": {},
   "source": [
    "# Attention Rollout\n",
    "## During first forward pass when attn weights are input_token_len by input_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e93f55-a80e-4530-bdea-3904cd85ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout_function(attn_maps):\n",
    "    attn_rollout = []\n",
    "    device = attn_maps[0].device\n",
    "    batch_size, _, seq_len, _ = attn_maps[0].shape\n",
    "    \n",
    "    # Identity matrix for self-attention\n",
    "    I = torch.eye(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len, seq_len)\n",
    "\n",
    "    prod = I.clone()\n",
    "    \n",
    "    for i, attn_map in enumerate(attn_maps):\n",
    "        # Average over heads → [batch, seq_len, seq_len]\n",
    "        attn_map = attn_map.mean(dim=1)\n",
    "        \n",
    "        # Add identity and multiply\n",
    "        prod = prod @ (attn_map + I)\n",
    "        \n",
    "        # Normalize across sequence dimension\n",
    "        prod = prod / prod.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        attn_rollout.append(prod)\n",
    "\n",
    "    return attn_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c46ebc-abde-44e0-b0d9-dad13fb7d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights[32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da006731-5802-4e26-818b-2751018587d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = attention_rollout_function(model.enc_attn_weights[0:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d532b3-7364-4803-9b5b-8d222a3cc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout[31][0] # Rollout up to i-th index layer, batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8788ee-862b-444c-b684-bf2b6e899c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals, topk_inds = torch.topk(rollout[31][0, 595], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b310-714d-47be-90e5-a43b8f8a5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe02a6-1d02-4236-9fec-d6d7437df3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in topk_inds:\n",
    "    print(repr(decoded_tokens[i.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330f9d3-40de-40d2-bf4a-e24a62e05b8b",
   "metadata": {},
   "source": [
    "# Visualise attention for subsequent forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f18fa5-066d-4414-b89e-63a270dc4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights[32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a73cdf-e7d5-4cd3-9d70-7527eee880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weight = model.enc_attn_weights[243][0].mean(dim=0)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b58925-f5fd-42a6-8e0b-1f9592244959",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals, topk_inds = torch.topk(attn_weight, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b198b-1e42-47ad-a858-95fba1006671",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567771ca-bf43-49f5-b9d7-1753e40a0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aba5c-991a-4838-8ca3-1d4fe05a8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = [0 for i in range(576)]\n",
    "\n",
    "for i in topk_inds[0]:\n",
    "    if repr(decoded_tokens[i.item()]) == \"'<image>'\":\n",
    "        heatmap[i.item()-5] = 1\n",
    "\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = torch.tensor(heatmap)\n",
    "heatmap = heatmap.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='nearest-exact')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec8161-7810-491f-8856-1cd596f25e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model starts generating tokens after \"...ASSISTANT:\"\n",
    "attn_avg = attn_weight\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[0, 5:581]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd7b3e-c8c0-45bc-9ce5-ff54fa5d2f21",
   "metadata": {},
   "source": [
    "# Attention relevancy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d667de-9320-4be4-b16f-a16d6b1ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "input_ids = inputs.input_ids\n",
    "output_ids = output.sequences.reshape(-1)[input_ids.shape[-1]:].tolist() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for target_index in tqdm(range(len(output.scores)), desc=\"Building relevancy maps\"):\n",
    "    token_logits = output.scores[target_index]\n",
    "    token_id = torch.tensor(output_ids[target_index]).to(device)\n",
    "\n",
    "    token_id_one_hot = torch.nn.functional.one_hot(token_id, num_classes=token_logits.size(-1)).float()\n",
    "    token_id_one_hot = token_id_one_hot.view(1, -1)\n",
    "    token_id_one_hot.requires_grad_(True)\n",
    "\n",
    "    # Compute loss and backpropagate to get gradients on attention weights\n",
    "    model.zero_grad()\n",
    "    token_logits.backward(gradient=token_id_one_hot)\n",
    "\n",
    "    for i, blk in enumerate(model.enc_attn_weights):\n",
    "        grad = blk.grad.float().detach()\n",
    "        print(grad)\n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e51cab-d6d2-4c70-be3e-f17771fcecc0",
   "metadata": {},
   "source": [
    "# Hidden states/concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12bb92-751d-4eba-b343-012626cc9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No. of forward passes: {len(output.hidden_states)}\")\n",
    "print(f\"No. of hidden states in each forward pass: {len(output.hidden_states[0])}\")\n",
    "print(f\"Shape of 1 hidden state in first forward pass: {output.hidden_states[0][24].shape}\")\n",
    "\n",
    "logits = model.lm_head(output.hidden_states[3][17])\n",
    "# Decode them\n",
    "topk = torch.topk(logits, k=10, dim=-1)\n",
    "for ids in topk.indices:\n",
    "    print(processor.tokenizer.batch_decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d3a81-fb35-4db2-a9c7-39493db07118",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62661e08-9c4d-4c43-bb96-ef8584e9993e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
