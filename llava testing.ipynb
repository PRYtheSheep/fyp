{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f9a306-ce41-4ff3-bb63-7f828d65d67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dreamcore\\anaconda3\\envs\\fyp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "func_to_enable_grad = '_sample'\n",
    "setattr(LlavaForConditionalGeneration, func_to_enable_grad, torch.enable_grad(getattr(LlavaForConditionalGeneration, func_to_enable_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c76156-9194-45c1-b771-5db7c48864db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 25.18it/s]\n",
      "C:\\Users\\Dreamcore\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n",
      "Fetching 2 files: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  \n",
      "What is the colour of this object? ASSISTANT: The object is red in colour.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation = \"eager\"\n",
    ").to(0)\n",
    "\n",
    "#--------------------------------------------------\n",
    "model.vision_tower.config.output_attentions = True\n",
    "\n",
    "# set hooks to get attention weights\n",
    "model.enc_attn_weights = []\n",
    "#outputs: attn_output, attn_weights, past_key_value\n",
    "def forward_hook(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.error(\n",
    "            (\"Attention weights were not returned for the encoder. \"\n",
    "            \"To enable, set output_attentions=True in the forward pass of the model. \")\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder, hooks_encoder = [], []\n",
    "for layer in model.language_model.layers:\n",
    "    hook_encoder_layer = layer.self_attn.register_forward_hook(forward_hook)\n",
    "    hooks_pre_encoder.append(hook_encoder_layer)\n",
    "\n",
    "model.enc_attn_weights_vit = []\n",
    "\n",
    "def forward_hook_image_processor(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.warning(\n",
    "            (\"Attention weights were not returned for the vision model. \"\n",
    "             \"Relevancy maps will not be calculated for the vision model. \" \n",
    "             \"To enable, set output_attentions=True in the forward pass of vision_tower. \")\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights_vit.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder_vit = []\n",
    "for layer in model.vision_tower.vision_model.encoder.layers:\n",
    "    hook_encoder_layer_vit = layer.self_attn.register_forward_hook(forward_hook_image_processor)\n",
    "    hooks_pre_encoder_vit.append(hook_encoder_layer_vit)\n",
    "#--------------------------------------------------\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "if model.language_model.config.model_type == \"gemma\":\n",
    "    eos_token_id = processor.tokenizer('<end_of_turn>', add_special_tokens=False).input_ids[0]\n",
    "else:\n",
    "    eos_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is the colour of this object?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "image_file = r\"C:\\Users\\Dreamcore\\Downloads\\61-gPGG7umL._UF894,1000_QL80_.jpg\"\n",
    "raw_image = Image.open(image_file).convert(\"RGB\")\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=200, \n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    eos_token_id=eos_token_id\n",
    ")\n",
    "\n",
    "for h in hooks_pre_encoder:\n",
    "    h.remove()\n",
    "for h in hooks_pre_encoder_vit:\n",
    "    h.remove()\n",
    "\n",
    "print(processor.decode(output.sequences[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b194a8-8b2c-4a63-b6e5-4f3588184afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '<s>'\n",
      "1: 'US'\n",
      "2: 'ER'\n",
      "3: ':'\n",
      "4: ''\n",
      "5: '<image>'\n",
      "6: '<image>'\n",
      "7: '<image>'\n",
      "8: '<image>'\n",
      "9: '<image>'\n",
      "10: '<image>'\n",
      "11: '<image>'\n",
      "12: '<image>'\n",
      "13: '<image>'\n",
      "14: '<image>'\n",
      "15: '<image>'\n",
      "16: '<image>'\n",
      "17: '<image>'\n",
      "18: '<image>'\n",
      "19: '<image>'\n",
      "20: '<image>'\n",
      "21: '<image>'\n",
      "22: '<image>'\n",
      "23: '<image>'\n",
      "24: '<image>'\n",
      "25: '<image>'\n",
      "26: '<image>'\n",
      "27: '<image>'\n",
      "28: '<image>'\n",
      "29: '<image>'\n",
      "30: '<image>'\n",
      "31: '<image>'\n",
      "32: '<image>'\n",
      "33: '<image>'\n",
      "34: '<image>'\n",
      "35: '<image>'\n",
      "36: '<image>'\n",
      "37: '<image>'\n",
      "38: '<image>'\n",
      "39: '<image>'\n",
      "40: '<image>'\n",
      "41: '<image>'\n",
      "42: '<image>'\n",
      "43: '<image>'\n",
      "44: '<image>'\n",
      "45: '<image>'\n",
      "46: '<image>'\n",
      "47: '<image>'\n",
      "48: '<image>'\n",
      "49: '<image>'\n",
      "50: '<image>'\n",
      "51: '<image>'\n",
      "52: '<image>'\n",
      "53: '<image>'\n",
      "54: '<image>'\n",
      "55: '<image>'\n",
      "56: '<image>'\n",
      "57: '<image>'\n",
      "58: '<image>'\n",
      "59: '<image>'\n",
      "60: '<image>'\n",
      "61: '<image>'\n",
      "62: '<image>'\n",
      "63: '<image>'\n",
      "64: '<image>'\n",
      "65: '<image>'\n",
      "66: '<image>'\n",
      "67: '<image>'\n",
      "68: '<image>'\n",
      "69: '<image>'\n",
      "70: '<image>'\n",
      "71: '<image>'\n",
      "72: '<image>'\n",
      "73: '<image>'\n",
      "74: '<image>'\n",
      "75: '<image>'\n",
      "76: '<image>'\n",
      "77: '<image>'\n",
      "78: '<image>'\n",
      "79: '<image>'\n",
      "80: '<image>'\n",
      "81: '<image>'\n",
      "82: '<image>'\n",
      "83: '<image>'\n",
      "84: '<image>'\n",
      "85: '<image>'\n",
      "86: '<image>'\n",
      "87: '<image>'\n",
      "88: '<image>'\n",
      "89: '<image>'\n",
      "90: '<image>'\n",
      "91: '<image>'\n",
      "92: '<image>'\n",
      "93: '<image>'\n",
      "94: '<image>'\n",
      "95: '<image>'\n",
      "96: '<image>'\n",
      "97: '<image>'\n",
      "98: '<image>'\n",
      "99: '<image>'\n",
      "100: '<image>'\n",
      "101: '<image>'\n",
      "102: '<image>'\n",
      "103: '<image>'\n",
      "104: '<image>'\n",
      "105: '<image>'\n",
      "106: '<image>'\n",
      "107: '<image>'\n",
      "108: '<image>'\n",
      "109: '<image>'\n",
      "110: '<image>'\n",
      "111: '<image>'\n",
      "112: '<image>'\n",
      "113: '<image>'\n",
      "114: '<image>'\n",
      "115: '<image>'\n",
      "116: '<image>'\n",
      "117: '<image>'\n",
      "118: '<image>'\n",
      "119: '<image>'\n",
      "120: '<image>'\n",
      "121: '<image>'\n",
      "122: '<image>'\n",
      "123: '<image>'\n",
      "124: '<image>'\n",
      "125: '<image>'\n",
      "126: '<image>'\n",
      "127: '<image>'\n",
      "128: '<image>'\n",
      "129: '<image>'\n",
      "130: '<image>'\n",
      "131: '<image>'\n",
      "132: '<image>'\n",
      "133: '<image>'\n",
      "134: '<image>'\n",
      "135: '<image>'\n",
      "136: '<image>'\n",
      "137: '<image>'\n",
      "138: '<image>'\n",
      "139: '<image>'\n",
      "140: '<image>'\n",
      "141: '<image>'\n",
      "142: '<image>'\n",
      "143: '<image>'\n",
      "144: '<image>'\n",
      "145: '<image>'\n",
      "146: '<image>'\n",
      "147: '<image>'\n",
      "148: '<image>'\n",
      "149: '<image>'\n",
      "150: '<image>'\n",
      "151: '<image>'\n",
      "152: '<image>'\n",
      "153: '<image>'\n",
      "154: '<image>'\n",
      "155: '<image>'\n",
      "156: '<image>'\n",
      "157: '<image>'\n",
      "158: '<image>'\n",
      "159: '<image>'\n",
      "160: '<image>'\n",
      "161: '<image>'\n",
      "162: '<image>'\n",
      "163: '<image>'\n",
      "164: '<image>'\n",
      "165: '<image>'\n",
      "166: '<image>'\n",
      "167: '<image>'\n",
      "168: '<image>'\n",
      "169: '<image>'\n",
      "170: '<image>'\n",
      "171: '<image>'\n",
      "172: '<image>'\n",
      "173: '<image>'\n",
      "174: '<image>'\n",
      "175: '<image>'\n",
      "176: '<image>'\n",
      "177: '<image>'\n",
      "178: '<image>'\n",
      "179: '<image>'\n",
      "180: '<image>'\n",
      "181: '<image>'\n",
      "182: '<image>'\n",
      "183: '<image>'\n",
      "184: '<image>'\n",
      "185: '<image>'\n",
      "186: '<image>'\n",
      "187: '<image>'\n",
      "188: '<image>'\n",
      "189: '<image>'\n",
      "190: '<image>'\n",
      "191: '<image>'\n",
      "192: '<image>'\n",
      "193: '<image>'\n",
      "194: '<image>'\n",
      "195: '<image>'\n",
      "196: '<image>'\n",
      "197: '<image>'\n",
      "198: '<image>'\n",
      "199: '<image>'\n",
      "200: '<image>'\n",
      "201: '<image>'\n",
      "202: '<image>'\n",
      "203: '<image>'\n",
      "204: '<image>'\n",
      "205: '<image>'\n",
      "206: '<image>'\n",
      "207: '<image>'\n",
      "208: '<image>'\n",
      "209: '<image>'\n",
      "210: '<image>'\n",
      "211: '<image>'\n",
      "212: '<image>'\n",
      "213: '<image>'\n",
      "214: '<image>'\n",
      "215: '<image>'\n",
      "216: '<image>'\n",
      "217: '<image>'\n",
      "218: '<image>'\n",
      "219: '<image>'\n",
      "220: '<image>'\n",
      "221: '<image>'\n",
      "222: '<image>'\n",
      "223: '<image>'\n",
      "224: '<image>'\n",
      "225: '<image>'\n",
      "226: '<image>'\n",
      "227: '<image>'\n",
      "228: '<image>'\n",
      "229: '<image>'\n",
      "230: '<image>'\n",
      "231: '<image>'\n",
      "232: '<image>'\n",
      "233: '<image>'\n",
      "234: '<image>'\n",
      "235: '<image>'\n",
      "236: '<image>'\n",
      "237: '<image>'\n",
      "238: '<image>'\n",
      "239: '<image>'\n",
      "240: '<image>'\n",
      "241: '<image>'\n",
      "242: '<image>'\n",
      "243: '<image>'\n",
      "244: '<image>'\n",
      "245: '<image>'\n",
      "246: '<image>'\n",
      "247: '<image>'\n",
      "248: '<image>'\n",
      "249: '<image>'\n",
      "250: '<image>'\n",
      "251: '<image>'\n",
      "252: '<image>'\n",
      "253: '<image>'\n",
      "254: '<image>'\n",
      "255: '<image>'\n",
      "256: '<image>'\n",
      "257: '<image>'\n",
      "258: '<image>'\n",
      "259: '<image>'\n",
      "260: '<image>'\n",
      "261: '<image>'\n",
      "262: '<image>'\n",
      "263: '<image>'\n",
      "264: '<image>'\n",
      "265: '<image>'\n",
      "266: '<image>'\n",
      "267: '<image>'\n",
      "268: '<image>'\n",
      "269: '<image>'\n",
      "270: '<image>'\n",
      "271: '<image>'\n",
      "272: '<image>'\n",
      "273: '<image>'\n",
      "274: '<image>'\n",
      "275: '<image>'\n",
      "276: '<image>'\n",
      "277: '<image>'\n",
      "278: '<image>'\n",
      "279: '<image>'\n",
      "280: '<image>'\n",
      "281: '<image>'\n",
      "282: '<image>'\n",
      "283: '<image>'\n",
      "284: '<image>'\n",
      "285: '<image>'\n",
      "286: '<image>'\n",
      "287: '<image>'\n",
      "288: '<image>'\n",
      "289: '<image>'\n",
      "290: '<image>'\n",
      "291: '<image>'\n",
      "292: '<image>'\n",
      "293: '<image>'\n",
      "294: '<image>'\n",
      "295: '<image>'\n",
      "296: '<image>'\n",
      "297: '<image>'\n",
      "298: '<image>'\n",
      "299: '<image>'\n",
      "300: '<image>'\n",
      "301: '<image>'\n",
      "302: '<image>'\n",
      "303: '<image>'\n",
      "304: '<image>'\n",
      "305: '<image>'\n",
      "306: '<image>'\n",
      "307: '<image>'\n",
      "308: '<image>'\n",
      "309: '<image>'\n",
      "310: '<image>'\n",
      "311: '<image>'\n",
      "312: '<image>'\n",
      "313: '<image>'\n",
      "314: '<image>'\n",
      "315: '<image>'\n",
      "316: '<image>'\n",
      "317: '<image>'\n",
      "318: '<image>'\n",
      "319: '<image>'\n",
      "320: '<image>'\n",
      "321: '<image>'\n",
      "322: '<image>'\n",
      "323: '<image>'\n",
      "324: '<image>'\n",
      "325: '<image>'\n",
      "326: '<image>'\n",
      "327: '<image>'\n",
      "328: '<image>'\n",
      "329: '<image>'\n",
      "330: '<image>'\n",
      "331: '<image>'\n",
      "332: '<image>'\n",
      "333: '<image>'\n",
      "334: '<image>'\n",
      "335: '<image>'\n",
      "336: '<image>'\n",
      "337: '<image>'\n",
      "338: '<image>'\n",
      "339: '<image>'\n",
      "340: '<image>'\n",
      "341: '<image>'\n",
      "342: '<image>'\n",
      "343: '<image>'\n",
      "344: '<image>'\n",
      "345: '<image>'\n",
      "346: '<image>'\n",
      "347: '<image>'\n",
      "348: '<image>'\n",
      "349: '<image>'\n",
      "350: '<image>'\n",
      "351: '<image>'\n",
      "352: '<image>'\n",
      "353: '<image>'\n",
      "354: '<image>'\n",
      "355: '<image>'\n",
      "356: '<image>'\n",
      "357: '<image>'\n",
      "358: '<image>'\n",
      "359: '<image>'\n",
      "360: '<image>'\n",
      "361: '<image>'\n",
      "362: '<image>'\n",
      "363: '<image>'\n",
      "364: '<image>'\n",
      "365: '<image>'\n",
      "366: '<image>'\n",
      "367: '<image>'\n",
      "368: '<image>'\n",
      "369: '<image>'\n",
      "370: '<image>'\n",
      "371: '<image>'\n",
      "372: '<image>'\n",
      "373: '<image>'\n",
      "374: '<image>'\n",
      "375: '<image>'\n",
      "376: '<image>'\n",
      "377: '<image>'\n",
      "378: '<image>'\n",
      "379: '<image>'\n",
      "380: '<image>'\n",
      "381: '<image>'\n",
      "382: '<image>'\n",
      "383: '<image>'\n",
      "384: '<image>'\n",
      "385: '<image>'\n",
      "386: '<image>'\n",
      "387: '<image>'\n",
      "388: '<image>'\n",
      "389: '<image>'\n",
      "390: '<image>'\n",
      "391: '<image>'\n",
      "392: '<image>'\n",
      "393: '<image>'\n",
      "394: '<image>'\n",
      "395: '<image>'\n",
      "396: '<image>'\n",
      "397: '<image>'\n",
      "398: '<image>'\n",
      "399: '<image>'\n",
      "400: '<image>'\n",
      "401: '<image>'\n",
      "402: '<image>'\n",
      "403: '<image>'\n",
      "404: '<image>'\n",
      "405: '<image>'\n",
      "406: '<image>'\n",
      "407: '<image>'\n",
      "408: '<image>'\n",
      "409: '<image>'\n",
      "410: '<image>'\n",
      "411: '<image>'\n",
      "412: '<image>'\n",
      "413: '<image>'\n",
      "414: '<image>'\n",
      "415: '<image>'\n",
      "416: '<image>'\n",
      "417: '<image>'\n",
      "418: '<image>'\n",
      "419: '<image>'\n",
      "420: '<image>'\n",
      "421: '<image>'\n",
      "422: '<image>'\n",
      "423: '<image>'\n",
      "424: '<image>'\n",
      "425: '<image>'\n",
      "426: '<image>'\n",
      "427: '<image>'\n",
      "428: '<image>'\n",
      "429: '<image>'\n",
      "430: '<image>'\n",
      "431: '<image>'\n",
      "432: '<image>'\n",
      "433: '<image>'\n",
      "434: '<image>'\n",
      "435: '<image>'\n",
      "436: '<image>'\n",
      "437: '<image>'\n",
      "438: '<image>'\n",
      "439: '<image>'\n",
      "440: '<image>'\n",
      "441: '<image>'\n",
      "442: '<image>'\n",
      "443: '<image>'\n",
      "444: '<image>'\n",
      "445: '<image>'\n",
      "446: '<image>'\n",
      "447: '<image>'\n",
      "448: '<image>'\n",
      "449: '<image>'\n",
      "450: '<image>'\n",
      "451: '<image>'\n",
      "452: '<image>'\n",
      "453: '<image>'\n",
      "454: '<image>'\n",
      "455: '<image>'\n",
      "456: '<image>'\n",
      "457: '<image>'\n",
      "458: '<image>'\n",
      "459: '<image>'\n",
      "460: '<image>'\n",
      "461: '<image>'\n",
      "462: '<image>'\n",
      "463: '<image>'\n",
      "464: '<image>'\n",
      "465: '<image>'\n",
      "466: '<image>'\n",
      "467: '<image>'\n",
      "468: '<image>'\n",
      "469: '<image>'\n",
      "470: '<image>'\n",
      "471: '<image>'\n",
      "472: '<image>'\n",
      "473: '<image>'\n",
      "474: '<image>'\n",
      "475: '<image>'\n",
      "476: '<image>'\n",
      "477: '<image>'\n",
      "478: '<image>'\n",
      "479: '<image>'\n",
      "480: '<image>'\n",
      "481: '<image>'\n",
      "482: '<image>'\n",
      "483: '<image>'\n",
      "484: '<image>'\n",
      "485: '<image>'\n",
      "486: '<image>'\n",
      "487: '<image>'\n",
      "488: '<image>'\n",
      "489: '<image>'\n",
      "490: '<image>'\n",
      "491: '<image>'\n",
      "492: '<image>'\n",
      "493: '<image>'\n",
      "494: '<image>'\n",
      "495: '<image>'\n",
      "496: '<image>'\n",
      "497: '<image>'\n",
      "498: '<image>'\n",
      "499: '<image>'\n",
      "500: '<image>'\n",
      "501: '<image>'\n",
      "502: '<image>'\n",
      "503: '<image>'\n",
      "504: '<image>'\n",
      "505: '<image>'\n",
      "506: '<image>'\n",
      "507: '<image>'\n",
      "508: '<image>'\n",
      "509: '<image>'\n",
      "510: '<image>'\n",
      "511: '<image>'\n",
      "512: '<image>'\n",
      "513: '<image>'\n",
      "514: '<image>'\n",
      "515: '<image>'\n",
      "516: '<image>'\n",
      "517: '<image>'\n",
      "518: '<image>'\n",
      "519: '<image>'\n",
      "520: '<image>'\n",
      "521: '<image>'\n",
      "522: '<image>'\n",
      "523: '<image>'\n",
      "524: '<image>'\n",
      "525: '<image>'\n",
      "526: '<image>'\n",
      "527: '<image>'\n",
      "528: '<image>'\n",
      "529: '<image>'\n",
      "530: '<image>'\n",
      "531: '<image>'\n",
      "532: '<image>'\n",
      "533: '<image>'\n",
      "534: '<image>'\n",
      "535: '<image>'\n",
      "536: '<image>'\n",
      "537: '<image>'\n",
      "538: '<image>'\n",
      "539: '<image>'\n",
      "540: '<image>'\n",
      "541: '<image>'\n",
      "542: '<image>'\n",
      "543: '<image>'\n",
      "544: '<image>'\n",
      "545: '<image>'\n",
      "546: '<image>'\n",
      "547: '<image>'\n",
      "548: '<image>'\n",
      "549: '<image>'\n",
      "550: '<image>'\n",
      "551: '<image>'\n",
      "552: '<image>'\n",
      "553: '<image>'\n",
      "554: '<image>'\n",
      "555: '<image>'\n",
      "556: '<image>'\n",
      "557: '<image>'\n",
      "558: '<image>'\n",
      "559: '<image>'\n",
      "560: '<image>'\n",
      "561: '<image>'\n",
      "562: '<image>'\n",
      "563: '<image>'\n",
      "564: '<image>'\n",
      "565: '<image>'\n",
      "566: '<image>'\n",
      "567: '<image>'\n",
      "568: '<image>'\n",
      "569: '<image>'\n",
      "570: '<image>'\n",
      "571: '<image>'\n",
      "572: '<image>'\n",
      "573: '<image>'\n",
      "574: '<image>'\n",
      "575: '<image>'\n",
      "576: '<image>'\n",
      "577: '<image>'\n",
      "578: '<image>'\n",
      "579: '<image>'\n",
      "580: '<image>'\n",
      "581: ''\n",
      "582: '\\n'\n",
      "583: 'What'\n",
      "584: 'is'\n",
      "585: 'the'\n",
      "586: 'colour'\n",
      "587: 'of'\n",
      "588: 'this'\n",
      "589: 'object'\n",
      "590: '?'\n",
      "591: 'A'\n",
      "592: 'SS'\n",
      "593: 'IST'\n",
      "594: 'ANT'\n",
      "595: ':'\n",
      "596: 'The'\n",
      "597: 'object'\n",
      "598: 'is'\n",
      "599: 'red'\n",
      "600: 'in'\n",
      "601: 'colour'\n",
      "602: '.'\n",
      "603: '</s>'\n"
     ]
    }
   ],
   "source": [
    "# output.sequences[0] is a tensor of shape [604]\n",
    "tokens = output.sequences[0].tolist()  # convert to Python list\n",
    "torch.save(output.sequences[0], \"full_generation.pt\")\n",
    "\n",
    "# Decode each token individually\n",
    "decoded_tokens = [processor.decode([t]) for t in tokens]\n",
    "\n",
    "# Optionally, print\n",
    "for i, tok in enumerate(decoded_tokens):\n",
    "    print(f\"{i}: {repr(tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26994886-41f9-4672-9d34-cc1e96cd3550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text', 'text': 'What is the colour of this object?'},\n",
       "   {'type': 'image'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text', 'text': 'The object is red in'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_step = conversation + [\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": processor.decode(tokens[596:601])}]}\n",
    "]\n",
    "conv_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e45108c4-b765-418c-a0d0-6fc38d6717d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del processor\n",
    "del hooks_pre_encoder\n",
    "del hooks_encoder\n",
    "del prompt\n",
    "del inputs\n",
    "del output\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef5187c-474e-4829-927d-96cc7c141b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33712128\n",
      "65011712\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bf5f7b-05ab-4813-a36b-ede5a960d21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 24.62it/s]\n",
      "Fetching 2 files: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    attn_implementation = \"eager\"\n",
    ").to(0)\n",
    "\n",
    "#--------------------------------------------------\n",
    "model.vision_tower.config.output_attentions = True\n",
    "\n",
    "# set hooks to get attention weights\n",
    "model.enc_attn_weights = []\n",
    "#outputs: attn_output, attn_weights, past_key_value\n",
    "def forward_hook(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.error(\n",
    "            (\"Attention weights were not returned for the encoder. \"\n",
    "            \"To enable, set output_attentions=True in the forward pass of the model. \")\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder, hooks_encoder = [], []\n",
    "for layer in model.language_model.layers:\n",
    "    hook_encoder_layer = layer.self_attn.register_forward_hook(forward_hook)\n",
    "    hooks_pre_encoder.append(hook_encoder_layer)\n",
    "\n",
    "model.enc_attn_weights_vit = []\n",
    "\n",
    "def forward_hook_image_processor(module, inputs, output): \n",
    "    if output[1] is None:\n",
    "        logger.warning(\n",
    "            (\"Attention weights were not returned for the vision model. \"\n",
    "             \"Relevancy maps will not be calculated for the vision model. \" \n",
    "             \"To enable, set output_attentions=True in the forward pass of vision_tower. \")\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    output[1].requires_grad_(True)\n",
    "    output[1].retain_grad()\n",
    "    model.enc_attn_weights_vit.append(output[1].detach().cpu())\n",
    "    return output\n",
    "\n",
    "hooks_pre_encoder_vit = []\n",
    "for layer in model.vision_tower.vision_model.encoder.layers:\n",
    "    hook_encoder_layer_vit = layer.self_attn.register_forward_hook(forward_hook_image_processor)\n",
    "    hooks_pre_encoder_vit.append(hook_encoder_layer_vit)\n",
    "#--------------------------------------------------\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "prompt_appended = processor.apply_chat_template(conv_step, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "inputs_appended = processor(images=raw_image, text=prompt_appended, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "print(len(model.enc_attn_weights))\n",
    "output_appended = model(**inputs_appended,\n",
    "                        use_cache=True,\n",
    "                        output_attentions=True,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                        eos_token_id=eos_token_id)\n",
    "print(len(model.enc_attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c7fea3-e172-4cec-afee-fe9df7a5e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 602, 32064])\n",
      "['colour']\n"
     ]
    }
   ],
   "source": [
    "print(output_appended.logits.shape)\n",
    "# Decode them\n",
    "topk = torch.topk(output_appended.logits[:,-2], k=1, dim=-1)\n",
    "for ids in topk.indices:\n",
    "    print(processor.tokenizer.batch_decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a83cc-286d-49c4-9293-3d06a43272f1",
   "metadata": {},
   "source": [
    "# Visualise ViT attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88826e80-2fa8-4ff7-b981-8973a3d09a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights_vit[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1101f19-ac79-424a-aa54-7296b7d843c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "attn_avg = model.enc_attn_weights_vit[3][0].mean(dim=0)\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[0, 1:]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3e24c-1587-4f49-875f-713f3039a954",
   "metadata": {},
   "source": [
    "# Visualize LLM attention \n",
    "## During first forward pass when attn weights are input_token_len by input_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d7419-b19b-4306-a602-549331c598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21212358-832a-42c3-9bf2-8b41b63a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"attn_weights has length: {len(model.enc_attn_weights)}\")\n",
    "print(f\"no. of forward passes = {len(model.enc_attn_weights)}/32 = {int(len(model.enc_attn_weights)/32)}\")\n",
    "\n",
    "# First forward pass, first layer\n",
    "print(model.enc_attn_weights[0].shape) # [batch, head, no. of tokens, no. of tokens]\n",
    "\n",
    "print(model.enc_attn_weights[51].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd8201-b121-49af-943b-a9ebddbd4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model starts generating tokens after \"...ASSISTANT:\"\n",
    "attn_avg = model.enc_attn_weights[11][0].mean(dim=0)\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[592, 5:581]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d44c2e-9a7b-44eb-887d-476b99b71417",
   "metadata": {},
   "source": [
    "# Attention Rollout\n",
    "## During first forward pass when attn weights are input_token_len by input_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e93f55-a80e-4530-bdea-3904cd85ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout_function(attn_maps):\n",
    "    attn_rollout = []\n",
    "    device = attn_maps[0].device\n",
    "    batch_size, _, seq_len, _ = attn_maps[0].shape\n",
    "    \n",
    "    # Identity matrix for self-attention\n",
    "    I = torch.eye(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len, seq_len)\n",
    "\n",
    "    prod = I.clone()\n",
    "    \n",
    "    for i, attn_map in enumerate(attn_maps):\n",
    "        # Average over heads → [batch, seq_len, seq_len]\n",
    "        attn_map = attn_map.mean(dim=1)\n",
    "        \n",
    "        # Add identity and multiply\n",
    "        prod = prod @ (attn_map + I)\n",
    "        \n",
    "        # Normalize across sequence dimension\n",
    "        prod = prod / prod.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        attn_rollout.append(prod)\n",
    "\n",
    "    return attn_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c46ebc-abde-44e0-b0d9-dad13fb7d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights[32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da006731-5802-4e26-818b-2751018587d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = attention_rollout_function(model.enc_attn_weights[0:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d532b3-7364-4803-9b5b-8d222a3cc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout[31][0] # Rollout up to i-th index layer, batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8788ee-862b-444c-b684-bf2b6e899c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals, topk_inds = torch.topk(rollout[31][0, 595], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b310-714d-47be-90e5-a43b8f8a5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe02a6-1d02-4236-9fec-d6d7437df3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in topk_inds:\n",
    "    print(repr(decoded_tokens[i.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330f9d3-40de-40d2-bf4a-e24a62e05b8b",
   "metadata": {},
   "source": [
    "# Visualise attention for subsequent forward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f18fa5-066d-4414-b89e-63a270dc4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_attn_weights[32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a73cdf-e7d5-4cd3-9d70-7527eee880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weight = model.enc_attn_weights[243][0].mean(dim=0)\n",
    "attn_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b58925-f5fd-42a6-8e0b-1f9592244959",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals, topk_inds = torch.topk(attn_weight, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b198b-1e42-47ad-a858-95fba1006671",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567771ca-bf43-49f5-b9d7-1753e40a0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aba5c-991a-4838-8ca3-1d4fe05a8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = [0 for i in range(576)]\n",
    "\n",
    "for i in topk_inds[0]:\n",
    "    if repr(decoded_tokens[i.item()]) == \"'<image>'\":\n",
    "        heatmap[i.item()-5] = 1\n",
    "\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = torch.tensor(heatmap)\n",
    "heatmap = heatmap.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='nearest-exact')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec8161-7810-491f-8856-1cd596f25e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model starts generating tokens after \"...ASSISTANT:\"\n",
    "attn_avg = attn_weight\n",
    "print(attn_avg.shape)\n",
    "cls_attn = attn_avg[0, 5:581]  # exclude CLS itself\n",
    "H, W = 24, 24  # 336 / 14\n",
    "heatmap = cls_attn.reshape(H, W).detach().cpu().numpy()\n",
    "\n",
    "# Assuming heatmap shape [H, W]\n",
    "heatmap_tensor = torch.tensor(heatmap[None, None], dtype=torch.float32)\n",
    "heatmap_full = F.interpolate(heatmap_tensor, size=(raw_image.size[1], raw_image.size[0]), mode='bilinear')[0,0].numpy()\n",
    "\n",
    "image = Image.open(image_file).convert(\"RGB\")\n",
    "plt.imshow(image)\n",
    "plt.imshow(heatmap_full, cmap='jet', alpha=0.75)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd7b3e-c8c0-45bc-9ce5-ff54fa5d2f21",
   "metadata": {},
   "source": [
    "# Attention relevancy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d667de-9320-4be4-b16f-a16d6b1ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "input_ids = inputs.input_ids\n",
    "output_ids = output.sequences.reshape(-1)[input_ids.shape[-1]:].tolist() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for target_index in tqdm(range(len(output.scores)), desc=\"Building relevancy maps\"):\n",
    "    token_logits = output.scores[target_index]\n",
    "    token_id = torch.tensor(output_ids[target_index]).to(device)\n",
    "\n",
    "    token_id_one_hot = torch.nn.functional.one_hot(token_id, num_classes=token_logits.size(-1)).float()\n",
    "    token_id_one_hot = token_id_one_hot.view(1, -1)\n",
    "    token_id_one_hot.requires_grad_(True)\n",
    "\n",
    "    # Compute loss and backpropagate to get gradients on attention weights\n",
    "    model.zero_grad()\n",
    "    token_logits.backward(gradient=token_id_one_hot)\n",
    "\n",
    "    for i, blk in enumerate(model.enc_attn_weights):\n",
    "        grad = blk.grad.float().detach()\n",
    "        print(grad)\n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e51cab-d6d2-4c70-be3e-f17771fcecc0",
   "metadata": {},
   "source": [
    "# Hidden states/concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12bb92-751d-4eba-b343-012626cc9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No. of forward passes: {len(output.hidden_states)}\")\n",
    "print(f\"No. of hidden states in each forward pass: {len(output.hidden_states[0])}\")\n",
    "print(f\"Shape of 1 hidden state in first forward pass: {output.hidden_states[0][24].shape}\")\n",
    "\n",
    "logits = model.lm_head(output.hidden_states[3][17])\n",
    "# Decode them\n",
    "topk = torch.topk(logits, k=10, dim=-1)\n",
    "for ids in topk.indices:\n",
    "    print(processor.tokenizer.batch_decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d3a81-fb35-4db2-a9c7-39493db07118",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62661e08-9c4d-4c43-bb96-ef8584e9993e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
